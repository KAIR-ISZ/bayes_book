%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]



\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{My sample book}
\date{Mar 03, 2022}
\release{}
\author{The Jupyter Book Community}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{intro::doc}}


\sphinxAtStartPar
This is a small sample book to give you a feel for how book content is
structured.
It shows off a few of the major file types, as well as some sample content.
It does not go in\sphinxhyphen{}depth into any particular topic \sphinxhyphen{} check out \sphinxhref{https://jupyterbook.org}{the Jupyter Book documentation} for more information.

\sphinxAtStartPar
Check out the content pages bundled with this sample book to see more.
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{markdown::doc}]{\sphinxcrossref{Markdown Files}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{notebooks::doc}]{\sphinxcrossref{Content with notebooks}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{markdown-notebooks::doc}]{\sphinxcrossref{Notebooks with MyST Markdown}}}

\end{itemize}


\chapter{Markdown Files}
\label{\detokenize{markdown:markdown-files}}\label{\detokenize{markdown::doc}}
\sphinxAtStartPar
Whether you write your book’s content in Jupyter Notebooks (\sphinxcode{\sphinxupquote{.ipynb}}) or
in regular markdown files (\sphinxcode{\sphinxupquote{.md}}), you’ll write in the same flavor of markdown
called \sphinxstylestrong{MyST Markdown}.
This is a simple file to help you get started and show off some syntax.


\section{What is MyST?}
\label{\detokenize{markdown:what-is-myst}}
\sphinxAtStartPar
MyST stands for “Markedly Structured Text”. It
is a slight variation on a flavor of markdown called “CommonMark” markdown,
with small syntax extensions to allow you to write \sphinxstylestrong{roles} and \sphinxstylestrong{directives}
in the Sphinx ecosystem.

\sphinxAtStartPar
For more about MyST, see \sphinxhref{https://jupyterbook.org/content/myst.html}{the MyST Markdown Overview}.


\section{Sample Roles and Directivs}
\label{\detokenize{markdown:sample-roles-and-directivs}}
\sphinxAtStartPar
Roles and directives are two of the most powerful tools in Jupyter Book. They
are kind of like functions, but written in a markup language. They both
serve a similar purpose, but \sphinxstylestrong{roles are written in one line}, whereas
\sphinxstylestrong{directives span many lines}. They both accept different kinds of inputs,
and what they do with those inputs depends on the specific role or directive
that is being called.

\sphinxAtStartPar
Here is a “note” directive:

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Here is a note
\end{sphinxadmonition}

\sphinxAtStartPar
It will be rendered in a special box when you build your book.

\sphinxAtStartPar
Here is an inline directive to refer to a document: {\hyperref[\detokenize{markdown-notebooks::doc}]{\sphinxcrossref{\DUrole{doc}{Notebooks with MyST Markdown}}}}.


\section{Citations}
\label{\detokenize{markdown:citations}}
\sphinxAtStartPar
You can also cite references that are stored in a \sphinxcode{\sphinxupquote{bibtex}} file. For example,
the following syntax: \sphinxcode{\sphinxupquote{\{cite\}`holdgraf\_evidence\_2014`}} will render like
this: {[}\hyperlink{cite.markdown:id3}{HdHPK14}{]}.

\sphinxAtStartPar
Moreover, you can insert a bibliography into your page with this syntax:
The \sphinxcode{\sphinxupquote{\{bibliography\}}} directive must be used for all the \sphinxcode{\sphinxupquote{\{cite\}}} roles to
render properly.
For example, if the references for your book are stored in \sphinxcode{\sphinxupquote{references.bib}},
then the bibliography is inserted with:

\sphinxAtStartPar



\section{Learn more}
\label{\detokenize{markdown:learn-more}}
\sphinxAtStartPar
This is just a simple starter to get you started.
You can learn a lot more at \sphinxhref{https://jupyterbook.org}{jupyterbook.org}.


\section{Data analytics}
\label{\detokenize{Lecture 1:data-analytics}}\label{\detokenize{Lecture 1::doc}}

\subsection{Intro to statistical modelling}
\label{\detokenize{Lecture 1:intro-to-statistical-modelling}}

\subsubsection{dr hab. inż. Jerzy Baranowski, Prof. AGH}
\label{\detokenize{Lecture 1:dr-hab-inz-jerzy-baranowski-prof-agh}}

\subsection{Organizational aspects}
\label{\detokenize{Lecture 1:organizational-aspects}}\begin{itemize}
\item {} 
\sphinxAtStartPar
C3 214,

\item {} 
\sphinxAtStartPar
\sphinxhref{mailto:jb@agh.edu.pl}{jb@agh.edu.pl}

\item {} 
\sphinxAtStartPar
Final grade: 50\% exam, 50\% laboratories/project. Passing of laboratories required for attempting exam.

\item {} 
\sphinxAtStartPar
Exam in oral form (list of topics will be available)

\item {} 
\sphinxAtStartPar
Course materials, including lecture slides are available on \sphinxhref{https://github.com/KAIR-ISZ/public\_lectures/tree/master/Data\%20Analytics}{Github}, if you see something wrong \sphinxhyphen{} make an issue.

\end{itemize}


\subsection{Course code}
\label{\detokenize{Lecture 1:course-code}}

\subsubsection{EAIiIBAiRCSS.IIi1O.34b7b98ba1017eb4b851f46d7326974f.19}
\label{\detokenize{Lecture 1:eaiiibaircss-iii1o-34b7b98ba1017eb4b851f46d7326974f-19}}

\subsection{Course outline}
\label{\detokenize{Lecture 1:course-outline}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Introduction to Statistical Modeling

\item {} 
\sphinxAtStartPar
General concepts of Bayesian paradigm

\item {} 
\sphinxAtStartPar
Monte Carlo computation

\item {} 
\sphinxAtStartPar
Simple models and uncertanity

\item {} 
\sphinxAtStartPar
Bayesian workflow

\item {} 
\sphinxAtStartPar
Causality in models

\item {} 
\sphinxAtStartPar
Hierarchical and multilevel models

\item {} 
\sphinxAtStartPar
Model checking

\item {} 
\sphinxAtStartPar
Modeling of missing data

\end{itemize}


\subsection{Source material \sphinxhyphen{} BDA 3rd edition}
\label{\detokenize{Lecture 1:source-material-bda-3rd-edition}}
\sphinxAtStartPar
Andrew Gelman et. Al


\begin{itemize}
\item {} 
\sphinxAtStartPar
Examples in R but easily transferable.

\item {} 
\sphinxAtStartPar
Great lectures by Prof. Aki Vehtari from Alto University \sphinxhref{https://aalto.cloud.panopto.eu/Panopto/Pages/Sessions/List.aspx\#folderID=\%22f0ec3a25-9e23-4935-873b-a9f401646812\%22}{link}

\end{itemize}


\subsection{Source material \sphinxhyphen{} Statistical Rethinking 2nd edition}
\label{\detokenize{Lecture 1:source-material-statistical-rethinking-2nd-edition}}
\sphinxAtStartPar
Richard McElreath


\begin{itemize}
\item {} 
\sphinxAtStartPar
Probably the best source for self learning of Bayesian methods

\item {} 
\sphinxAtStartPar
More code oriented, also in R, lots of additional resources, including Youtube lectures available \sphinxhref{https://xcelab.net/rm/statistical-rethinking/}{here}

\end{itemize}


\subsection{Source material \sphinxhyphen{} case studies by Michael Betancourt}
\label{\detokenize{Lecture 1:source-material-case-studies-by-michael-betancourt}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Available \sphinxhref{https://betanalpha.github.io/writing/}{here}

\item {} 
\sphinxAtStartPar
More advanced, but at the same time cutting edge

\item {} 
\sphinxAtStartPar
Some content (most recent and podcasts) is available only for patrons

\end{itemize}


\subsection{Extra reading \sphinxhyphen{} Student’s guide to Bayesian statistics}
\label{\detokenize{Lecture 1:extra-reading-student-s-guide-to-bayesian-statistics}}
\sphinxAtStartPar
Ben Lambert



\sphinxAtStartPar
Second “modern” book on Bayesian statistics


\subsection{Extra reading \sphinxhyphen{} Bayesian analysis with Python}
\label{\detokenize{Lecture 1:extra-reading-bayesian-analysis-with-python}}
\sphinxAtStartPar
Osvaldo Martin



\sphinxAtStartPar
Actually only book based on Python, with focus on PyMC3. You need to know what you are doing. Risky choice for first contact.


\subsection{Interesting people on Twitter}
\label{\detokenize{Lecture 1:interesting-people-on-twitter}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Prof Aki Vehtari \sphinxhref{https://twitter.com/avehtari}{@avehtari}

\item {} 
\sphinxAtStartPar
Prof. Andrew Gelman \sphinxhref{https://twitter.com/StatModeling}{@StatModeling} \sphinxhyphen{} also has an interesting blog

\item {} 
\sphinxAtStartPar
Michael Betancourt \sphinxhref{https://twitter.com/betanalpha}{@betanalpha}

\item {} 
\sphinxAtStartPar
Richard McElreath \sphinxhref{https://twitter.com/rlmcelreath}{@rlmcelreath}

\item {} 
\sphinxAtStartPar
Alex Andorra \sphinxhref{https://twitter.com/alex\_andorra}{@alex\_andorra} \sphinxhyphen{} podcast on Bayesian statisticsa

\item {} 
\sphinxAtStartPar
there will be a twitter list available on Github

\end{itemize}


\subsection{What is the point?}
\label{\detokenize{Lecture 1:what-is-the-point}}\begin{itemize}
\item {} 
\sphinxAtStartPar
We are focusing on Bayesian Data Analysis and statistical modelling

\item {} 
\sphinxAtStartPar
Models grounded in probability

\item {} 
\sphinxAtStartPar
As interpretable as possible

\item {} 
\sphinxAtStartPar
Maximally transparent

\end{itemize}


\subsection{What is data science?}
\label{\detokenize{Lecture 1:what-is-data-science}}


\sphinxAtStartPar
Work of data scientist intertwines both machine learning and statistical modelling and multiple other fields


\subsection{SM vs ML}
\label{\detokenize{Lecture 1:sm-vs-ml}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Statistical modelling and machine learning are closely related fields, often hard to distinguish

\item {} 
\sphinxAtStartPar
They should not be directly compared because those comparisons are usually unfair to one or another, as they are for different problems

\end{itemize}


\subsection{SM}
\label{\detokenize{Lecture 1:sm}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Incorporates probability

\item {} 
\sphinxAtStartPar
Considers data generation

\item {} 
\sphinxAtStartPar
Looks for interpretability

\item {} 
\sphinxAtStartPar
Usually regression based

\item {} 
\sphinxAtStartPar
Not limited to linear

\end{itemize}


\subsection{ML}
\label{\detokenize{Lecture 1:ml}}\begin{itemize}
\item {} 
\sphinxAtStartPar
No initial structure nor traditional parameters

\item {} 
\sphinxAtStartPar
No focus on single variable

\item {} 
\sphinxAtStartPar
Does not model the process but learns from data

\item {} 
\sphinxAtStartPar
Does not rely on additivity

\end{itemize}


\subsection{Advantages of ML}
\label{\detokenize{Lecture 1:advantages-of-ml}}\begin{itemize}
\item {} 
\sphinxAtStartPar
ML is the best in high Signal/Noise ratios

\item {} 
\sphinxAtStartPar
Especially visual and sound recognition, language translation

\item {} 
\sphinxAtStartPar
More of a black box approach

\item {} 
\sphinxAtStartPar
Large datasets with multiple number of features

\end{itemize}


\subsection{Advantages of SM}
\label{\detokenize{Lecture 1:advantages-of-sm}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Handles small datasets better

\item {} 
\sphinxAtStartPar
Provides uncertainty estimates

\item {} 
\sphinxAtStartPar
Transparent

\item {} 
\sphinxAtStartPar
Allows investigation of influence of predictors

\end{itemize}


\subsection{Discussion points}
\label{\detokenize{Lecture 1:discussion-points}}\begin{itemize}
\item {} 
\sphinxAtStartPar
ML might need more data for the same problem as SM

\item {} 
\sphinxAtStartPar
SM needs interactions to be specified, while ML can determine them more freely

\item {} 
\sphinxAtStartPar
ML usually is a better classifier/predictor but uncertanity is not handled that well

\item {} 
\sphinxAtStartPar
ML has much more vocal advocates

\item {} 
\sphinxAtStartPar
SM requires data reduction for larger datasets

\end{itemize}


\subsection{When to use SM?}
\label{\detokenize{Lecture 1:when-to-use-sm}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Uncertainty is important or Signal/Noise ratio is small

\item {} 
\sphinxAtStartPar
Not perfect training data

\item {} 
\sphinxAtStartPar
Isolation of particular variables effects

\item {} 
\sphinxAtStartPar
Additivity

\item {} 
\sphinxAtStartPar
Smaller samples

\item {} 
\sphinxAtStartPar
Interpretability

\end{itemize}


\subsection{When to use ML?}
\label{\detokenize{Lecture 1:when-to-use-ml}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Signal/noise ratio is large and little randomness

\item {} 
\sphinxAtStartPar
Relatively unlimited training data

\item {} 
\sphinxAtStartPar
Overall prediction is important

\item {} 
\sphinxAtStartPar
Uncertanity is not

\item {} 
\sphinxAtStartPar
Expected substantial nonlinearity

\item {} 
\sphinxAtStartPar
Huge samples

\item {} 
\sphinxAtStartPar
Black box is acceptable

\end{itemize}


\subsection{Bayesian Statistical Modelling}
\label{\detokenize{Lecture 1:bayesian-statistical-modelling}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Three essential steps:

\item {} 
\sphinxAtStartPar
Set up full probability model

\item {} 
\sphinxAtStartPar
Condition on the observed data

\item {} 
\sphinxAtStartPar
Check and evaluate model and its posterior distribution (repeat if necessary)

\end{itemize}


\subsection{Bayesian paradigm}
\label{\detokenize{Lecture 1:bayesian-paradigm}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Bayesian statistics differs in two main points with frequentist statistics:

\item {} 
\sphinxAtStartPar
Data is fixed, parameters are uncertain

\item {} 
\sphinxAtStartPar
Prior knowledge is inconporated in inference

\item {} 
\sphinxAtStartPar
Everything has a probability distribution

\end{itemize}


\subsection{Bayesian methods work}
\label{\detokenize{Lecture 1:bayesian-methods-work}}
\sphinxAtStartPar
Sharon Bertsch McGrayne



\sphinxAtStartPar
Available e.g. on Audible (1st month free)
History of use of Bayesian statistics


\subsection{Main fields of Bayesian applications}
\label{\detokenize{Lecture 1:main-fields-of-bayesian-applications}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Social sciences

\item {} 
\sphinxAtStartPar
Medicine and biology

\item {} 
\sphinxAtStartPar
Experimental sciences

\item {} 
\sphinxAtStartPar
Diagnositics

\item {} 
\sphinxAtStartPar
Decision support

\end{itemize}


\subsection{Main concepts of BDA}
\label{\detokenize{Lecture 1:main-concepts-of-bda}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Observables and unobservables

\item {} 
\sphinxAtStartPar
Parameters \(\theta\), data \(y\) and predictions \(\hat{y}\)

\item {} 
\sphinxAtStartPar
Observational units and variables

\item {} 
\sphinxAtStartPar
Exchangeability

\item {} 
\sphinxAtStartPar
Explanatory variables (predictors)

\item {} 
\sphinxAtStartPar
Hierarchical modelling

\item {} 
\sphinxAtStartPar
Utility distributions

\end{itemize}


\subsection{Bayes’ rule derivation}
\label{\detokenize{Lecture 1:bayes-rule-derivation}}
\sphinxAtStartPar
It all starts with joint probability
\begin{equation*}
\begin{split}p(\theta,y)=p(\theta)p(y|\theta)\end{split}
\end{equation*}
\sphinxAtStartPar
With relatively basic transformations
\begin{equation*}
\begin{split} p(\theta|y)=\frac{p(\theta,y)}{p(y)}=\frac{p(\theta)p(y|\theta)}{p(y)}\end{split}
\end{equation*}
\sphinxAtStartPar
where
\begin{equation*}
\begin{split}p(y)=\sum_\theta p(\theta)p(y|\theta)\end{split}
\end{equation*}

\subsection{Bayes’ rule}
\label{\detokenize{Lecture 1:bayes-rule}}
\sphinxAtStartPar
In most applicatations we focus on the numerator
\begin{equation*}
\begin{split} \underbrace{p(\theta|y)}_{\mathrm{posterior}}\propto 
\underbrace{p(\theta)}_{\mathrm{prior}}
\underbrace{p(y|\theta)}_{\mathrm{likelihood}}\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxincludegraphics{{quote}.png}


\subsection{Example – spelling correction (BDA3)}
\label{\detokenize{Lecture 1:example-spelling-correction-bda3}}
\sphinxAtStartPar
Probability of writing ‘radom’ instead of ‘random’
\begin{equation*}
\begin{split}
p(\theta|y=\mathrm{'radom'})\propto p(\theta)p(y=\mathrm{'radom'}|\theta)
\end{split}
\end{equation*}
\sphinxAtStartPar
Simplifying assumptions – only 3 candidates \(\theta_1=\mathrm{'random'}\), \(\theta_2=\mathrm{'radon'}\) and \(\theta_3=\mathrm{'radom'}\)
\begin{equation*}
\begin{split}
p(\theta_1|y=\mathrm{'radom'}))= \frac{p(\theta_1)p(y=\mathrm{'radom'}|\theta_1)}{\sum_{j=1}^{3}p(\theta_j)p(y=\mathrm{'radom'}|\theta_j)}
\end{split}
\end{equation*}

\subsection{Example cont.}
\label{\detokenize{Lecture 1:example-cont}}
\sphinxAtStartPar
Data comes from Google spellcheck model


\subsubsection{Prior distribution}
\label{\detokenize{Lecture 1:prior-distribution}}

\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\theta\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(p(\theta)\)
\\
\hline
\sphinxAtStartPar
random
&
\sphinxAtStartPar
\(7.60\ \times\ 10^{−5}\)
\\
\hline
\sphinxAtStartPar
radon
&
\sphinxAtStartPar
\(6.05\ \times\ 10^{−6}\)
\\
\hline
\sphinxAtStartPar
radom
&
\sphinxAtStartPar
\(3.12\ \times\ 10^{−7}\)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsection{Example cont.}
\label{\detokenize{Lecture 1:id1}}

\subsubsection{Likelyhood}
\label{\detokenize{Lecture 1:likelyhood}}

\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\theta\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(p(\mathrm{radom}\vert\theta)\)
\\
\hline
\sphinxAtStartPar
random
&
\sphinxAtStartPar
0.00193
\\
\hline
\sphinxAtStartPar
radon
&
\sphinxAtStartPar
0.000143000
\\
\hline
\sphinxAtStartPar
radom
&
\sphinxAtStartPar
0.975
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsection{Example cont.}
\label{\detokenize{Lecture 1:id2}}
\sphinxAtStartPar
Posterior distribution


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\theta\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(p(\theta)p(\mathrm{'radom'}\vert\theta)\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(p(\theta\vert\mathrm{'radom'})\)
\\
\hline
\sphinxAtStartPar
random
&
\sphinxAtStartPar
\(1.47\ \times\ 10^{−7}\)
&
\sphinxAtStartPar
\(0.325000000\)
\\
\hline
\sphinxAtStartPar
radon
&
\sphinxAtStartPar
\(8.65\ \times\ 10^{−10}\)
&
\sphinxAtStartPar
\(0.002000000\)
\\
\hline
\sphinxAtStartPar
radom
&
\sphinxAtStartPar
\(3.04\ \times\ 10^{−7}\)
&
\sphinxAtStartPar
\(0.673000000\)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\section{Data analytics}
\label{\detokenize{Lecture 2:data-analytics}}\label{\detokenize{Lecture 2::doc}}

\subsection{Bayesian basics}
\label{\detokenize{Lecture 2:bayesian-basics}}

\subsubsection{dr hab. inż. Jerzy Baranowski, Prof. AGH}
\label{\detokenize{Lecture 2:dr-hab-inz-jerzy-baranowski-prof-agh}}

\subsection{Bayes’ rule}
\label{\detokenize{Lecture 2:bayes-rule}}
\sphinxAtStartPar
In most applicatations we focus on the numerator
\begin{equation*}
\begin{split} \underbrace{p(\theta|y)}_{\mathrm{posterior}}\propto 
\underbrace{p(\theta)}_{\mathrm{prior}}
\underbrace{p(y|\theta)}_{\mathrm{likelihood}}
\end{split}
\end{equation*}

\subsection{Predictions}
\label{\detokenize{Lecture 2:predictions}}

\subsubsection{Prior predictive distribution}
\label{\detokenize{Lecture 2:prior-predictive-distribution}}
\sphinxAtStartPar
What values of data can we expect before actual measurements (based on prior knowledge)
\begin{equation*}
\begin{split}
p(y)=\int p(y,\theta) d\theta=\int p(\theta)p(y|\theta)d\theta
\end{split}
\end{equation*}

\subsection{Predictions}
\label{\detokenize{Lecture 2:id1}}

\subsubsection{Posterior predictive distribution}
\label{\detokenize{Lecture 2:posterior-predictive-distribution}}
\sphinxAtStartPar
What values of new data can we expect based on previous measurements and prior knowledge
\begin{equation*}
\begin{split}
p(\tilde{y}|y)=\int p(\tilde{y},\theta|y) d\theta=
\int p(\tilde{y}|\theta,y) p(\theta|y)d\theta=
\int p(\tilde{y}|\theta) p(\theta|y)d\theta
\end{split}
\end{equation*}

\subsection{Additional notation and properties}
\label{\detokenize{Lecture 2:additional-notation-and-properties}}

\subsubsection{Everything is conditional}
\label{\detokenize{Lecture 2:everything-is-conditional}}\begin{equation*}
\begin{split}p(\theta,y|H)=p(\theta|H)p(y|\theta,H)\end{split}
\end{equation*}

\subsubsection{Expectation}
\label{\detokenize{Lecture 2:expectation}}\begin{equation*}
\begin{split} \mathrm{E}(u)=\int up(u) d u \end{split}
\end{equation*}

\subsubsection{Variance}
\label{\detokenize{Lecture 2:variance}}\begin{equation*}
\begin{split} \mathrm{var}(u)=\int(u-\mathrm{E}(u))(u-\mathrm{E}(u))^T p(u) du\end{split}
\end{equation*}

\subsection{Conditional expectations and variances}
\label{\detokenize{Lecture 2:conditional-expectations-and-variances}}

\subsubsection{Expectation of conditional distribution}
\label{\detokenize{Lecture 2:expectation-of-conditional-distribution}}\begin{equation*}
\begin{split} \mathrm{E}(u)=\mathrm{E}(\mathrm{E}(u|v)) \end{split}
\end{equation*}\begin{equation*}
\begin{split} \mathrm{E}(u)=\iint u p(u,v) du dv =\iint u p(u|v) du p(v) dv=\int  \mathrm{E}(u|v)p(v)d v \end{split}
\end{equation*}

\subsubsection{Variance of conditional distribution}
\label{\detokenize{Lecture 2:variance-of-conditional-distribution}}\begin{equation*}
\begin{split} \mathrm{var}(u)=\mathrm{E}(\mathrm{var}(u|v))+\mathrm{var}(\mathrm{E}(u|v) \end{split}
\end{equation*}

\subsection{Changing variables}
\label{\detokenize{Lecture 2:changing-variables}}\begin{equation*}
\begin{split} v=f(u) \end{split}
\end{equation*}

\subsubsection{Discrete distributions}
\label{\detokenize{Lecture 2:discrete-distributions}}\begin{equation*}
\begin{split}
p_v(v)=p_u(f^{-1}(v))
\end{split}
\end{equation*}

\subsection{Continuous distributions}
\label{\detokenize{Lecture 2:continuous-distributions}}\begin{equation*}
\begin{split}
p_v(v)=|J|p_u(f^{-1}(v))
\end{split}
\end{equation*}

\subsection{Typical variable changes}
\label{\detokenize{Lecture 2:typical-variable-changes}}
\sphinxAtStartPar
It is useful to work on the unbounded interval i.e. \((-\infty,\infty)\). Often parameters are bounded, so we use transformations
\begin{itemize}
\item {} 
\sphinxAtStartPar
Logarithmic (from \((0,\infty)\rightarrow (-\infty,\infty)\))
\$\( v=\log(u)\)\$

\item {} 
\sphinxAtStartPar
Logistic (from \((0,1)\rightarrow (-\infty,\infty)\))
\$\( v=\mathrm{logit}(u) \)\(
where 
\)\( \mathrm{logit}(x)=\log\left(\frac{x}{1-x}\right),\quad
\mathrm{logit}^{-1}(y)=\frac{\exp(y)}{1+\exp(y)}\)\$

\item {} 
\sphinxAtStartPar
Probit (from \((0,1)\rightarrow (-\infty,\infty)\))
\$\( v=\Phi^{-1}(u) \)\(
where \)\textbackslash{}phi\$ is the standard normal cumulative distribution function

\end{itemize}


\subsection{Single parameter models}
\label{\detokenize{Lecture 2:single-parameter-models}}

\subsubsection{Building blocks of more complicated models}
\label{\detokenize{Lecture 2:building-blocks-of-more-complicated-models}}

\subsubsection{Answers to basic questions e.g.:}
\label{\detokenize{Lecture 2:answers-to-basic-questions-e-g}}\begin{itemize}
\item {} 
\sphinxAtStartPar
What is the average difference between treatment groups?

\item {} 
\sphinxAtStartPar
How strong is the association between a treatment and an outcome?

\item {} 
\sphinxAtStartPar
Does the effect of the treatment depend upon a covariate?

\item {} 
\sphinxAtStartPar
How much variation is there among groups?

\end{itemize}


\subsection{Binomial model}
\label{\detokenize{Lecture 2:binomial-model}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Natural model for data that arise from a sequence of \(n\) exchangeable trials

\item {} 
\sphinxAtStartPar
Two possible outcomes, conventionally labeled ‘success’ and ‘failure.’
\$\(
p(y|\theta)=\mathrm{Bin}(y|n,\theta)={n \choose y}\theta^y (1-\theta)^{n-y}
\)\$

\end{itemize}


\subsection{Example of Bayesian learning}
\label{\detokenize{Lecture 2:example-of-bayesian-learning}}
\sphinxAtStartPar
Globe tossing:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The true proportion of water covering the globe is p.

\item {} 
\sphinxAtStartPar
A single toss of the globe has a probability \(p\) of producing a water (W) observation.

\item {} 
\sphinxAtStartPar
It has a probability \(1 − p\) of producing a land (L) observation.

\item {} 
\sphinxAtStartPar
Each toss of the globe is independent of the others.

\end{itemize}


\subsection{Globe tossing likelyhood}
\label{\detokenize{Lecture 2:globe-tossing-likelyhood}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k}{as} \PYG{n+nn}{stats}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{c+c1}{\PYGZsh{}import seaborn as sns}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{theta}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{200}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{}sns.set(style=\PYGZsq{}darkgrid\PYGZsq{})}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{7}
\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{11}
\PYG{n}{likelihood} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{binom}\PYG{o}{.}\PYG{n}{pmf}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{n}{n}\PYG{p}{,} \PYG{n}{theta}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{theta}\PYG{p}{,}\PYG{n}{likelihood}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+sa}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}}\PYG{l+s+s1}{\PYGZbs{}}\PYG{l+s+s1}{theta\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Lecture 2_13_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{How does it look analytically?}
\label{\detokenize{Lecture 2:how-does-it-look-analytically}}
\sphinxAtStartPar
Posterior distribution with uniform prior takes form
\$\( p(\theta|y)\propto \theta^y (1-\theta)^{n-y}\)\(
Which has form of Beta distribution
\)\( \theta|y\sim \mathrm{Beta} (y+1,n-y+1)\)\$


\subsection{How to summarize posterior?}
\label{\detokenize{Lecture 2:how-to-summarize-posterior}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Ideally – by itself

\item {} 
\sphinxAtStartPar
Usually – mean, median, mode

\item {} 
\sphinxAtStartPar
Variation – standard deviation, the interquartile range, and other quantiles

\end{itemize}


\subsection{Interval summaries}
\label{\detokenize{Lecture 2:interval-summaries}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Central interval \sphinxhyphen{} a symmetric interval around for example a mean

\item {} 
\sphinxAtStartPar
Highest posterior density region \sphinxhyphen{} smallest interval containing the desired probability

\end{itemize}




\subsection{Priors}
\label{\detokenize{Lecture 2:priors}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{population interpretation}, the prior distribution represents a population of possible parameter values, from which the \(\theta\) of current interest has been drawn

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{state of knowledge interpretation}, we must express our knowledge (and uncertainty) about \(\theta\) as if its value could be thought of as a random realization from the prior distribution

\end{itemize}


\subsection{Informative priors}
\label{\detokenize{Lecture 2:informative-priors}}\begin{itemize}
\item {} 
\sphinxAtStartPar
They try to introduce new information

\item {} 
\sphinxAtStartPar
Often interpreted as knowledge from previous experiments

\item {} 
\sphinxAtStartPar
For binomial example
\$\( p(\theta)\propto \theta^{\alpha-1}(1-\theta)^{\beta-1}\)\(
It’s a Beta distribution \)\textbackslash{}theta \textbackslash{}sim \textbackslash{}mathrm\{Beta\}(\textbackslash{}alpha, \textbackslash{}beta)\(, prior density is equivalent to \)\textbackslash{}alpha − 1\( prior successes and \)\textbackslash{}beta − 1\$ prior failures.

\end{itemize}


\subsection{Posterior with informative prior}
\label{\detokenize{Lecture 2:posterior-with-informative-prior}}
\sphinxAtStartPar
Posterior has a closed form
\$\(
\begin{aligned}
p(\theta|y)\propto{}& \theta^{y}(1-\theta)^{n-y}\theta^{\alpha-1}(1-\theta)^{\beta-1}=\\
={}&\theta^{y+\alpha-1}(1-\theta)^{n-y+\beta-1}=\\
={}&\mathrm{Beta}(\alpha+y,\beta+n-y)
\end{aligned}
\)\(
With expectation
\)\(
\mathrm{E}(\theta|y)=\frac{\alpha+y}{\alpha+\beta+n}
\)\(
which always lies between the sample proportion, \)y/n\(, and the prior mean, \)\textbackslash{}alpha/(\textbackslash{}alpha + \textbackslash{}beta)\$


\subsection{Conjugacy}
\label{\detokenize{Lecture 2:conjugacy}}\begin{equation*}
\begin{split} p(\theta|y)\in\mathcal{P}\mathrm{for\ all}\ p(\cdot|\theta)\in\mathcal{F}\ \mathrm{and}\ p(\cdot)\in\mathcal{P}
\end{split}
\end{equation*}
\sphinxAtStartPar
Class \(\mathcal{P}\) is conjugate to class \(\mathcal{F}\)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Conjugacy makes everything easier, formulas are analytic so computation is faster

\item {} 
\sphinxAtStartPar
It should not be a goal by itself, as sometimes different distributions are much more justified

\end{itemize}


\subsection{Example \sphinxhyphen{} biology}
\label{\detokenize{Lecture 2:example-biology}}
\sphinxAtStartPar
Is proportion of girls born with placenta previa lower than the proportion of female births in general population i.e. 0.485?

\sphinxAtStartPar
Study results: of 980 births with PP 437 were female.



\subsection{Uniform prior}
\label{\detokenize{Lecture 2:uniform-prior}}
\sphinxAtStartPar
Assuming uniform prior the analytic computation gives us posterior Beta(438,544). We can compute values analytically but its easier via sampling

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{arviz} \PYG{k}{as} \PYG{n+nn}{az}
\PYG{n}{samples}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{beta}\PYG{p}{(}\PYG{l+m+mi}{438}\PYG{p}{,}\PYG{l+m+mi}{544}\PYG{p}{,}\PYG{l+m+mi}{10000}\PYG{p}{)}
\PYG{n}{interval}\PYG{o}{=}\PYG{n}{az}\PYG{o}{.}\PYG{n}{hdi}\PYG{p}{(}\PYG{n}{samples}\PYG{p}{,}\PYG{l+m+mf}{0.94}\PYG{p}{)} 

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{94}\PYG{l+s+s1}{\PYGZpc{}}\PYG{l+s+s1}{ HPD interval:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{+}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{interval}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Median:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{+}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{median}\PYG{p}{(}\PYG{n}{samples}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
94\PYGZpc{} HPD interval:[0.41688465 0.47647535]
Median:0.4461052564859803
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsubsection{Visualisation}
\label{\detokenize{Lecture 2:visualisation}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{theta1}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mf}{0.3}\PYG{p}{,}\PYG{l+m+mf}{0.6}\PYG{p}{,}\PYG{l+m+mi}{200}\PYG{p}{)}
\PYG{n}{theta2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{n}{interval}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{interval}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{l+m+mi}{200}\PYG{p}{)}
\PYG{n}{bd}\PYG{o}{=}\PYG{n}{stats}\PYG{o}{.}\PYG{n}{beta}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{,}\PYG{l+m+mi}{438}\PYG{p}{,}\PYG{l+m+mi}{544}\PYG{p}{)}
\PYG{n}{bd2}\PYG{o}{=}\PYG{n}{stats}\PYG{o}{.}\PYG{n}{beta}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{theta2}\PYG{p}{,}\PYG{l+m+mi}{438}\PYG{p}{,}\PYG{l+m+mi}{544}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{,}\PYG{n}{bd}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{fill\PYGZus{}between}\PYG{p}{(}\PYG{n}{theta2}\PYG{p}{,} \PYG{n}{bd2}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Lecture 2_25_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fig}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Lecture 2_26_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Conjugate priors}
\label{\detokenize{Lecture 2:conjugate-priors}}
\sphinxAtStartPar
For binomial likelihood Beta is its conjugate. We can encode our knowledge (mean value of whole population) using the following relationships:
\begin{itemize}
\item {} 
\sphinxAtStartPar
mean of \(\mathrm{Beta}(\alpha,\beta)\) distribution is \(\frac{\alpha}{\alpha+\beta}\)

\item {} 
\sphinxAtStartPar
interpreting Beta, as previous binomial experiments then \(\alpha+\beta\) is the population size

\item {} 
\sphinxAtStartPar
we can try to do computation using various population sizes keeping mean of \(0.485\)

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sizes}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{200}\PYG{p}{,} \PYG{l+m+mi}{1000}\PYG{p}{]}
\PYG{n}{post\PYGZus{}median}\PYG{o}{=}\PYG{p}{[}\PYG{p}{]}
\PYG{n}{post\PYGZus{}hpd}\PYG{o}{=}\PYG{p}{[}\PYG{p}{]}
\PYG{n}{alphas}\PYG{o}{=}\PYG{p}{[}\PYG{p}{]}
\PYG{n}{betas}\PYG{o}{=}\PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{n} \PYG{o+ow}{in} \PYG{n}{sizes}\PYG{p}{:}
    \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.485}\PYG{o}{*}\PYG{n}{n}
    \PYG{n}{beta}\PYG{o}{=}\PYG{n}{n}\PYG{o}{\PYGZhy{}}\PYG{n}{alpha}
    \PYG{n}{alphas}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{alpha}\PYG{p}{)}
    \PYG{n}{betas}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{beta}\PYG{p}{)}
    \PYG{n}{post\PYGZus{}samples}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{beta}\PYG{p}{(}\PYG{l+m+mi}{438}\PYG{o}{+}\PYG{n}{alpha}\PYG{p}{,}\PYG{l+m+mi}{544}\PYG{o}{+}\PYG{n}{beta}\PYG{p}{,}\PYG{l+m+mi}{1000}\PYG{p}{)}
    \PYG{n}{post\PYGZus{}median}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{median}\PYG{p}{(}\PYG{n}{post\PYGZus{}samples}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{post\PYGZus{}hpd}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{az}\PYG{o}{.}\PYG{n}{hdi}\PYG{p}{(}\PYG{n}{post\PYGZus{}samples}\PYG{p}{,}\PYG{l+m+mf}{0.94}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{n}{df}\PYG{o}{=}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Population size}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{sizes}\PYG{p}{,}
                 \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Posterior median}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{post\PYGZus{}median}\PYG{p}{,}
                 \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Posterior 94}\PYG{l+s+si}{\PYGZpc{} c}\PYG{l+s+s1}{redible interval}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{post\PYGZus{}hpd}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{7}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Population size  Posterior median  \PYGZbs{}
0                2          0.445856   
1                5          0.445527   
2               10          0.447407   
3               20          0.446807   
4              100          0.450309   
5              200          0.452851   
6             1000          0.465229   

             Posterior 94\PYGZpc{} credible interval  
0   [0.417484255691757, 0.47602979210304935]  
1  [0.41811418664120764, 0.4778747099232391]  
2  [0.4186888381864138, 0.47691604699679235]  
3  [0.4155422094988496, 0.47405246625526415]  
4   [0.4222400631569723, 0.4796459579519981]  
5   [0.4232550816547827, 0.4770771131716891]  
6  [0.44581209507534625, 0.4862551829261736]  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{f}\PYG{p}{,} \PYG{p}{(}\PYG{n}{ax1}\PYG{p}{,} \PYG{n}{ax2}\PYG{p}{,}\PYG{n}{ax3}\PYG{p}{)} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{sharex}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{7}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}\PYG{n}{tight\PYGZus{}layout}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{ax1}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{,}\PYG{n}{stats}\PYG{o}{.}\PYG{n}{beta}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{,}\PYG{n}{alphas}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{betas}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{theta1}\PYG{p}{,} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{beta}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{,}\PYG{l+m+mi}{438}\PYG{o}{+}\PYG{n}{alphas}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{l+m+mi}{544}\PYG{o}{+}\PYG{n}{betas}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{theta1}\PYG{p}{,} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{beta}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{,}\PYG{l+m+mi}{438}\PYG{p}{,}\PYG{l+m+mi}{544}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Prior population size=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{+}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{sizes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ax2}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{,}\PYG{n}{stats}\PYG{o}{.}\PYG{n}{beta}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{,}\PYG{n}{alphas}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}\PYG{n}{betas}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{theta1}\PYG{p}{,} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{beta}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{,}\PYG{l+m+mi}{438}\PYG{o}{+}\PYG{n}{alphas}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}\PYG{l+m+mi}{544}\PYG{o}{+}\PYG{n}{betas}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{theta1}\PYG{p}{,} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{beta}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{,}\PYG{l+m+mi}{438}\PYG{p}{,}\PYG{l+m+mi}{544}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{ax2}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Prior population size=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{+}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{sizes}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{ax2}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ax3}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{,}\PYG{n}{stats}\PYG{o}{.}\PYG{n}{beta}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{,}\PYG{n}{alphas}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{,}\PYG{n}{betas}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{theta1}\PYG{p}{,} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{beta}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{,}\PYG{l+m+mi}{438}\PYG{o}{+}\PYG{n}{alphas}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{,}\PYG{l+m+mi}{544}\PYG{o}{+}\PYG{n}{betas}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{theta1}\PYG{p}{,} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{beta}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{,}\PYG{l+m+mi}{438}\PYG{p}{,}\PYG{l+m+mi}{544}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{ax3}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Prior population size=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{+}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{sizes}\PYG{p}{[}\PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{ax3}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{f}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Prior}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Posterior}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Posterior with uniform prior}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Lecture 2_31_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{f}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Lecture 2_32_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Non\sphinxhyphen{}conjugate prior}
\label{\detokenize{Lecture 2:non-conjugate-prior}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{f2}\PYG{p}{,} \PYG{p}{(}\PYG{n}{ax1}\PYG{p}{,} \PYG{n}{ax2}\PYG{p}{,}\PYG{n}{ax3}\PYG{p}{)} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{sharex}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{7}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}\PYG{n}{tight\PYGZus{}layout}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{n}{ax1}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{,} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{beta}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{,}\PYG{l+m+mi}{438}\PYG{p}{,}\PYG{l+m+mi}{544}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Likelihood}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax2}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{,}\PYG{n}{stats}\PYG{o}{.}\PYG{n}{norm}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{,}\PYG{l+m+mf}{0.485}\PYG{p}{,}\PYG{l+m+mf}{0.05}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{ax2}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Nonconjugate prior: N(0.485,0.05)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{post}\PYG{o}{=}\PYG{n}{stats}\PYG{o}{.}\PYG{n}{beta}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{,}\PYG{l+m+mi}{438}\PYG{p}{,}\PYG{l+m+mi}{544}\PYG{p}{)}\PYG{o}{*}\PYG{n}{stats}\PYG{o}{.}\PYG{n}{norm}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{,}\PYG{l+m+mf}{0.485}\PYG{p}{,}\PYG{l+m+mf}{0.05}\PYG{p}{)}
\PYG{n}{post}\PYG{o}{=}\PYG{n}{post}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{post}\PYG{p}{)}
\PYG{n}{ax3}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{theta1}\PYG{p}{,} \PYG{n}{post}\PYG{p}{)}
\PYG{n}{ax3}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Non\PYGZhy{}conjugate posterior}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ax2}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ax3}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Lecture 2_34_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Data analytics}
\label{\detokenize{Lecture 3:data-analytics}}\label{\detokenize{Lecture 3::doc}}

\subsection{Exploring distributions}
\label{\detokenize{Lecture 3:exploring-distributions}}

\subsubsection{dr hab. inż. Jerzy Baranowski, Prof. AGH}
\label{\detokenize{Lecture 3:dr-hab-inz-jerzy-baranowski-prof-agh}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k}{as} \PYG{n+nn}{stats}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{arviz} \PYG{k}{as} \PYG{n+nn}{az}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{n}{light}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZsh{}FFFCDC}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{light\PYGZus{}highlight}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZsh{}FEF590}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{mid}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZsh{}FDED2A}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{mid\PYGZus{}highlight}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZsh{}f0dc05}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{dark}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZsh{}EECA02}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{dark\PYGZus{}highlight}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZsh{}BB9700}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{green}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZsh{}00FF00}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{light\PYGZus{}grey}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZsh{}DDDDDD}\PYG{l+s+s2}{\PYGZdq{}}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Managing expectations}
\label{\detokenize{Lecture 3:managing-expectations}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Main point of probabilistic compoutation is to compute an expectation of certain function of parameters.

\item {} 
\sphinxAtStartPar
It generally covers all kind of statistics

\item {} 
\sphinxAtStartPar
Has many beneficial properties

\end{itemize}


\subsection{Expectations}
\label{\detokenize{Lecture 3:expectations}}
\sphinxAtStartPar
In general case, function of parameter \(q\in Q\): \(f(q)\)  with respect to a probability distribution (mass function) \(\pi(q)\) has expectation given by
\begin{equation*}
\begin{split}
\mathbb{E}_{\pi}[f] = \int_{Q} \mathrm{d} q \, \pi(q) \, f(q).
\end{split}
\end{equation*}
\sphinxAtStartPar
or in discrete case
\begin{equation*}
\begin{split}
\mathbb{E}_{\pi}[f] = \sum_{q \in Q} \pi(q) \, f(q).
\end{split}
\end{equation*}

\subsection{How to compute expectations?}
\label{\detokenize{Lecture 3:how-to-compute-expectations}}
\sphinxAtStartPar
Analytic integration is practically impossible.

\sphinxAtStartPar
We are left with quadratures, for ex. Euler
\$\(
\mathbb{E}_{\pi}[f] \approx 
\sum_{n = 1}^{N} (\Delta q)_{n} \, \pi(q_{n}) \, f(q_{n}).
\)\$


\subsection{How to compute expectations?}
\label{\detokenize{Lecture 3:id1}}
\sphinxAtStartPar
Other option is exact sampling, leading to so called Monte Carlo estimators.

\sphinxAtStartPar
If we can generate set of samples \(\{ q_{1}, \ldots, q_{N} \} \in Q\), such that
\begin{equation*}
\begin{split}\hat{f}_{N}^{\text{MC}} = \frac{1}{N} \sum_{n = 1}^{N} f(q_{n}),\end{split}
\end{equation*}
\sphinxAtStartPar
asymptotically converges
\begin{equation*}
\begin{split}
\lim_{N \rightarrow \infty} \hat{f}_{N}^{\text{MC}} = \mathbb{E}_{\pi}[f].
\end{split}
\end{equation*}
\sphinxAtStartPar
Then we have an exact sampling procedure


\subsection{Monte Carlo estimators}
\label{\detokenize{Lecture 3:monte-carlo-estimators}}
\sphinxAtStartPar
Provided, that samples are generated properly we can quantify estimator error
\begin{equation*}
\begin{split}
\frac{ \hat{f}_{N}^{\text{MC}} - \mathbb{E}_{\pi}[f] }
{\text{MC-SE}_{N}[f] } 
\sim \mathcal{N}(0, 1),
\end{split}
\end{equation*}
\sphinxAtStartPar
With Monte Carlo Standard Error given by
\$\(
\text{MC-SE}_{N}[f] 
= \sqrt{ \frac{ \text{Var}_{\pi}[f]}{N} }.
\)\$


\subsection{Motivational example}
\label{\detokenize{Lecture 3:motivational-example}}\begin{itemize}
\item {} 
\sphinxAtStartPar
We return to Mass Effect experiment using grid approximation

\end{itemize}






\subsection{Grid approximation}
\label{\detokenize{Lecture 3:grid-approximation}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Define the grid. This means you decide how many points to use in estimating the posterior, and then you make a list of the parameter values on the grid.

\item {} 
\sphinxAtStartPar
Compute the value of the prior at each parameter value on the grid.

\item {} 
\sphinxAtStartPar
Compute the likelihood at each parameter value.

\item {} 
\sphinxAtStartPar
Compute the unstandardized posterior at each parameter value, by multiplying the prior by the likelihood.

\item {} 
\sphinxAtStartPar
Finally, standardize the posterior, by dividing each value by the sum of all values.

\end{enumerate}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{posterior\PYGZus{}grid\PYGZus{}approx}\PYG{p}{(}\PYG{n}{grid\PYGZus{}points}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{success}\PYG{o}{=}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{tosses}\PYG{o}{=}\PYG{l+m+mi}{9}\PYG{p}{)}\PYG{p}{:}
    
    \PYG{c+c1}{\PYGZsh{} define grid}
    \PYG{n}{p\PYGZus{}grid} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{grid\PYGZus{}points}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} define prior}
    \PYG{n}{prior} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{repeat}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{grid\PYGZus{}points}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} uniform}
   
    \PYG{c+c1}{\PYGZsh{} compute likelihood at each point in the grid}
    \PYG{n}{likelihood} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{binom}\PYG{o}{.}\PYG{n}{pmf}\PYG{p}{(}\PYG{n}{success}\PYG{p}{,} \PYG{n}{tosses}\PYG{p}{,} \PYG{n}{p\PYGZus{}grid}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} compute product of likelihood and prior}
    \PYG{n}{unstd\PYGZus{}posterior} \PYG{o}{=} \PYG{n}{likelihood} \PYG{o}{*} \PYG{n}{prior}

    \PYG{c+c1}{\PYGZsh{} standardize the posterior, so it sums to 1}
    \PYG{n}{posterior} \PYG{o}{=} \PYG{n}{unstd\PYGZus{}posterior} \PYG{o}{/} \PYG{n}{unstd\PYGZus{}posterior}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{p\PYGZus{}grid}\PYG{p}{,} \PYG{n}{posterior}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{posterior\PYGZus{}grid\PYGZus{}approx}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
(array([0.  , 0.25, 0.5 , 0.75, 1.  ]),
 array([0.        , 0.02129338, 0.40378549, 0.57492114, 0.        ]))
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Grid approximation example}
\label{\detokenize{Lecture 3:grid-approximation-example}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{f3}\PYG{p}{,} \PYG{p}{(}\PYG{n}{ax1}\PYG{p}{,} \PYG{n}{ax2}\PYG{p}{)} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{sharex}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{7}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}\PYG{n}{tight\PYGZus{}layout}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{grid5}\PYG{o}{=}\PYG{n}{posterior\PYGZus{}grid\PYGZus{}approx}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{grid20}\PYG{o}{=}\PYG{n}{posterior\PYGZus{}grid\PYGZus{}approx}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{ax1}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{grid5}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{grid5}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{o}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{color}\PYG{o}{=}\PYG{n}{dark}\PYG{p}{)}
\PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{5 points}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{probability of water}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{posterior probability}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ax2}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{grid20}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{grid20}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{o}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{color}\PYG{o}{=}\PYG{n}{dark}\PYG{p}{)}
\PYG{n}{ax2}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{20 points}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax2}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{probability of water}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax2}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Lecture 3_14_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{f3}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Lecture 3_15_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Summarizing by sampling}
\label{\detokenize{Lecture 3:summarizing-by-sampling}}\begin{itemize}
\item {} 
\sphinxAtStartPar
The easiest way to get information about even complicated posteriors is to simulate data that correspond to it and get parameter estimates from sampling.

\item {} 
\sphinxAtStartPar
For single parameter problems the easiest way is to use inverse cumulative distribution function

\item {} 
\sphinxAtStartPar
Grid approximation is more universal

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{posterior\PYGZus{}grid\PYGZus{}approx}\PYG{p}{(}\PYG{n}{grid\PYGZus{}points}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{success}\PYG{o}{=}\PYG{l+m+mi}{7}\PYG{p}{,} \PYG{n}{tosses}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{,}\PYG{n}{prior\PYGZus{}sel}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    
    \PYG{c+c1}{\PYGZsh{} define grid}
    \PYG{n}{p\PYGZus{}grid} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{grid\PYGZus{}points}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} define prior}
    \PYG{k}{if} \PYG{n}{prior\PYGZus{}sel}\PYG{o}{==}\PYG{l+m+mi}{1}\PYG{p}{:}
        \PYG{n}{prior} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{repeat}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{grid\PYGZus{}points}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} uniform}
    \PYG{k}{elif} \PYG{n}{prior\PYGZus{}sel}\PYG{o}{==}\PYG{l+m+mi}{2}\PYG{p}{:}
        \PYG{n}{prior} \PYG{o}{=} \PYG{p}{(}\PYG{n}{p\PYGZus{}grid} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} truncated}
    \PYG{k}{elif} \PYG{n}{prior\PYGZus{}sel}\PYG{o}{==}\PYG{l+m+mi}{3}\PYG{p}{:}    
        \PYG{n}{prior} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{5} \PYG{o}{*} \PYG{n+nb}{abs}\PYG{p}{(}\PYG{n}{p\PYGZus{}grid} \PYG{o}{\PYGZhy{}} \PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} double exp }
    \PYG{k}{else}\PYG{p}{:}
        \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Unsuported prior selection}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} compute likelihood at each point in the grid}
    \PYG{n}{likelihood} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{binom}\PYG{o}{.}\PYG{n}{pmf}\PYG{p}{(}\PYG{n}{success}\PYG{p}{,} \PYG{n}{tosses}\PYG{p}{,} \PYG{n}{p\PYGZus{}grid}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} compute product of likelihood and prior}
    \PYG{n}{unstd\PYGZus{}posterior} \PYG{o}{=} \PYG{n}{likelihood} \PYG{o}{*} \PYG{n}{prior}

    \PYG{c+c1}{\PYGZsh{} standardize the posterior, so it sums to 1}
    \PYG{n}{posterior} \PYG{o}{=} \PYG{n}{unstd\PYGZus{}posterior} \PYG{o}{/} \PYG{n}{unstd\PYGZus{}posterior}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{p\PYGZus{}grid}\PYG{p}{,} \PYG{n}{posterior}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p\PYGZus{}grid}\PYG{p}{,} \PYG{n}{posterior}\PYG{o}{=}\PYG{n}{posterior\PYGZus{}grid\PYGZus{}approx}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{,}\PYG{l+m+mi}{9}\PYG{p}{,}\PYG{n}{prior\PYGZus{}sel}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{p\PYGZus{}grid}\PYG{p}{,} \PYG{n}{prior}\PYG{o}{=}\PYG{n}{posterior\PYGZus{}grid\PYGZus{}approx}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{prior\PYGZus{}sel}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{fig}\PYG{p}{,}\PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{7}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}\PYG{n}{tight\PYGZus{}layout}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{p\PYGZus{}grid}\PYG{p}{,} \PYG{n}{posterior}\PYG{p}{,}\PYG{n}{color}\PYG{o}{=}\PYG{n}{dark}\PYG{p}{,}\PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Posterior}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{p\PYGZus{}grid}\PYG{p}{,} \PYG{n}{prior}\PYG{p}{,}\PYG{n}{color}\PYG{o}{=}\PYG{n}{mid}\PYG{p}{,}\PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Prior}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}


\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{probability of water}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ probability}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Lecture 3_18_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Sampling from grid posterior}
\label{\detokenize{Lecture 3:sampling-from-grid-posterior}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{44}\PYG{p}{)}
\PYG{n}{samples}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n}{p\PYGZus{}grid}\PYG{p}{,}\PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{10000}\PYG{p}{,}\PYG{n}{replace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}\PYG{n}{p}\PYG{o}{=}\PYG{n}{posterior}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,}\PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{7}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}\PYG{n}{tight\PYGZus{}layout}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10000}\PYG{p}{)}\PYG{p}{,}\PYG{n}{samples}\PYG{p}{,}\PYG{n}{color}\PYG{o}{=}\PYG{n}{dark}\PYG{p}{,}\PYG{n}{edgecolor}\PYG{o}{=}\PYG{n}{dark\PYGZus{}highlight}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sample number}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+sa}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{value of \PYGZdl{}}\PYG{l+s+s1}{\PYGZbs{}}\PYG{l+s+s1}{theta\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Lecture 3_20_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Weakly informative priors}
\label{\detokenize{Lecture 3:weakly-informative-priors}}
\sphinxAtStartPar
We characterize a prior distribution as weakly informative if it is proper but is set up so that the information it does provide is intentionally weaker than whatever actual prior knowledge is available.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Make uninformative more complicated

\item {} 
\sphinxAtStartPar
Make informative less complicated

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fig}\PYG{p}{,}\PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{7}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}\PYG{n}{tight\PYGZus{}layout}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{samples}\PYG{p}{,}\PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{25}\PYG{p}{,}\PYG{n}{density}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}\PYG{n}{color}\PYG{o}{=}\PYG{n}{dark}\PYG{p}{,}\PYG{n}{edgecolor}\PYG{o}{=}\PYG{n}{dark\PYGZus{}highlight}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+sa}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{value of \PYGZdl{}}\PYG{l+s+s1}{\PYGZbs{}}\PYG{l+s+s1}{theta\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Lecture 3_22_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} posterior probability where p \PYGZlt{} 0.5}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(} \PYG{n}{posterior}\PYG{p}{[} \PYG{n}{p\PYGZus{}grid} \PYG{o}{\PYGZlt{}} \PYG{l+m+mf}{0.5} \PYG{p}{]} \PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.24107415057429224
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} same by sampling}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(} \PYG{n}{samples} \PYG{o}{\PYGZlt{}} \PYG{l+m+mf}{0.5} \PYG{p}{)} \PYG{o}{/} \PYG{l+m+mf}{1e4}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.2371
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} intervals of interest}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(} \PYG{p}{(}\PYG{n}{samples} \PYG{o}{\PYGZgt{}} \PYG{l+m+mf}{0.5}\PYG{p}{)} \PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{samples} \PYG{o}{\PYGZlt{}} \PYG{l+m+mf}{0.75}\PYG{p}{)} \PYG{p}{)} \PYG{o}{/} \PYG{l+m+mf}{1e4}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.6694
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} quantiles}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{quantile}\PYG{p}{(}\PYG{n}{samples}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{0.1}\PYG{p}{,}\PYG{l+m+mf}{0.9}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([0.44444444, 0.74747475])
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Prior predictive distribution}
\label{\detokenize{Lecture 3:prior-predictive-distribution}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pr\PYGZus{}samples}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n}{p\PYGZus{}grid}\PYG{p}{,}\PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{10000}\PYG{p}{,}\PYG{n}{replace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}\PYG{n}{p}\PYG{o}{=}\PYG{n}{prior}\PYG{p}{)}
\PYG{n}{pr\PYGZus{}pr\PYGZus{}d\PYGZus{}samples}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{binomial}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{pr\PYGZus{}samples}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Mean rate of success = }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{pr\PYGZus{}pr\PYGZus{}d\PYGZus{}samples}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mf}{1e4}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Mean rate of success = 0.5017
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fig}\PYG{p}{,}\PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{7}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}\PYG{n}{tight\PYGZus{}layout}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{pr\PYGZus{}pr\PYGZus{}d\PYGZus{}samples}\PYG{p}{,}\PYG{n}{bins}\PYG{o}{=}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{.9}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{density}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}\PYG{n}{color}\PYG{o}{=}\PYG{n}{dark}\PYG{p}{,}\PYG{n}{edgecolor}\PYG{o}{=}\PYG{n}{dark\PYGZus{}highlight}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{outcome}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Lecture 3_29_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Posterior predictive distribution}
\label{\detokenize{Lecture 3:posterior-predictive-distribution}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{post\PYGZus{}samples}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n}{p\PYGZus{}grid}\PYG{p}{,}\PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{10000}\PYG{p}{,}\PYG{n}{replace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}\PYG{n}{p}\PYG{o}{=}\PYG{n}{posterior}\PYG{p}{)}
\PYG{n}{post\PYGZus{}pr\PYGZus{}d\PYGZus{}samples}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{binomial}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{post\PYGZus{}samples}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Mean rate of success = }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{post\PYGZus{}pr\PYGZus{}d\PYGZus{}samples}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mf}{1e4}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Mean rate of success = 0.5813
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fig}\PYG{p}{,}\PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{7}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}\PYG{n}{tight\PYGZus{}layout}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{post\PYGZus{}pr\PYGZus{}d\PYGZus{}samples}\PYG{p}{,}\PYG{n}{bins}\PYG{o}{=}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{.9}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{density}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}\PYG{n}{color}\PYG{o}{=}\PYG{n}{dark}\PYG{p}{,}\PYG{n}{edgecolor}\PYG{o}{=}\PYG{n}{dark\PYGZus{}highlight}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{outcome}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{Lecture 3_32_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{post\PYGZus{}pr\PYGZus{}d\PYGZus{}samples}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([1, 0, 1, ..., 0, 1, 1])
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Do grid approximations generalize?}
\label{\detokenize{Lecture 3:do-grid-approximations-generalize}}
\sphinxAtStartPar
It depends






\subsection{Sometimes geometry is difficult}
\label{\detokenize{Lecture 3:sometimes-geometry-is-difficult}}





\subsection{What contributes to expectation?}
\label{\detokenize{Lecture 3:what-contributes-to-expectation}}
\sphinxAtStartPar
Expectation is an integral
\begin{equation*}
\begin{split}
\mathbb{E}_{\pi}[f] = \int_{Q} \mathrm{d} q \, \pi(q) \, f(q).
\end{split}
\end{equation*}
\sphinxAtStartPar
Intuitively, wherever distribution \(\pi(q)\) is large, it should contribute the most, in particular next to maximum (mode).






\subsection{But what about the volume?}
\label{\detokenize{Lecture 3:but-what-about-the-volume}}
\sphinxAtStartPar
\(dq\) is also under the integral, and volume rises with dimension







\subsection{Volume rises exponentially with dimension}
\label{\detokenize{Lecture 3:volume-rises-exponentially-with-dimension}}





\subsection{It is the product that counts}
\label{\detokenize{Lecture 3:it-is-the-product-that-counts}}





\subsection{Typical set}
\label{\detokenize{Lecture 3:typical-set}}
\sphinxAtStartPar
What contributes the most to the expectation are the values from the typical set






\subsection{Concentration of measure}
\label{\detokenize{Lecture 3:concentration-of-measure}}
\sphinxAtStartPar
Typical set, is a “fuzzy surface” that is located progressively away from the mode with the rise of dimension.






\subsection{Typical set is where we should sample from}
\label{\detokenize{Lecture 3:typical-set-is-where-we-should-sample-from}}





\subsection{Computational algorithms for probabilistic computing}
\label{\detokenize{Lecture 3:computational-algorithms-for-probabilistic-computing}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Point estimators

\item {} 
\sphinxAtStartPar
Laplace approximation

\item {} 
\sphinxAtStartPar
Variational approximation

\item {} 
\sphinxAtStartPar
Monte Carlo estimators

\item {} 
\sphinxAtStartPar
Markov Chain Monte Carlo

\end{itemize}


\subsection{Modal estimators}
\label{\detokenize{Lecture 3:modal-estimators}}
\sphinxAtStartPar
This approach searches for the maximal value of probability distribution, in order to obtain approximation of expected value






\subsection{Issues}
\label{\detokenize{Lecture 3:issues}}\begin{itemize}
\item {} 
\sphinxAtStartPar
skewed distributions have maxima far from expectations

\item {} 
\sphinxAtStartPar
problems with uncertainty quantisation

\end{itemize}






\subsection{Laplace estimator}
\label{\detokenize{Lecture 3:laplace-estimator}}
\sphinxAtStartPar
Main idea is to find the maximal value, and fit a Gaussian distribution with a mean in it, and covariance obtained by second order Taylor approximation.

\sphinxAtStartPar
Expectation values can then estimated with Gaussian integrals,
\$\(
\mathbb{E}_{\pi} \! \left[ f \right]
\approx 
\int_{Q} \mathrm{d} q \, \mathcal{N} \! \left( q \mid \mu, \Sigma \right) \,
f \! \left( q \right),
\)\$


\subsection{If distribution is relatively close to Gaussian, typical set is well approximated}
\label{\detokenize{Lecture 3:if-distribution-is-relatively-close-to-gaussian-typical-set-is-well-approximated}}





\subsection{Variational approximation}
\label{\detokenize{Lecture 3:variational-approximation}}\begin{itemize}
\item {} 
\sphinxAtStartPar
The main idea is to approximate the posterior with functions, that can be easily sampled from (or their combination).

\item {} 
\sphinxAtStartPar
Such approximation is realized by minimization of a function called divergence, which measures how differetnt candidate and probability distribution are from one another.

\item {} 
\sphinxAtStartPar
In practice it is done by minimizing certain bound on the divergence.

\end{itemize}


\subsection{Multimodality of variational approximation}
\label{\detokenize{Lecture 3:multimodality-of-variational-approximation}}
\sphinxAtStartPar
It can happen, that significantly different candidates have similar divergences, that causes optimization problem to be multimodal






\subsection{Over and under fitting of the typical set}
\label{\detokenize{Lecture 3:over-and-under-fitting-of-the-typical-set}}



\subsection{Monte Carlo sampling}
\label{\detokenize{Lecture 3:monte-carlo-sampling}}





\subsection{Issues with Monte Carlo}
\label{\detokenize{Lecture 3:issues-with-monte-carlo}}\begin{itemize}
\item {} 
\sphinxAtStartPar
It is easy to sample from known distributions, especially low dimensional or normal

\item {} 
\sphinxAtStartPar
Complicated distributions are an issue

\item {} 
\sphinxAtStartPar
Importance sampling is an option
\begin{itemize}
\item {} 
\sphinxAtStartPar
Sample from something that you know (proposal distribution)

\item {} 
\sphinxAtStartPar
Correct with properly chosen weights

\item {} 
\sphinxAtStartPar
Strongly depends on quality of proposal

\end{itemize}

\end{itemize}


\subsection{Markov Chain Monte Carlo}
\label{\detokenize{Lecture 3:markov-chain-monte-carlo}}





\subsection{Extra reading}
\label{\detokenize{Lecture 3:extra-reading}}
\sphinxAtStartPar
\sphinxhref{https://betanalpha.github.io/assets/case\_studies/probabilistic\_computation.html\#1\_representation\_with\_computational\_taxation}{Probabilistic Computation by Michael Betancourt}


\chapter{Content with notebooks}
\label{\detokenize{notebooks:content-with-notebooks}}\label{\detokenize{notebooks::doc}}
\sphinxAtStartPar
You can also create content with Jupyter Notebooks. This means that you can include
code blocks and their outputs in your book.


\section{Markdown + notebooks}
\label{\detokenize{notebooks:markdown-notebooks}}
\sphinxAtStartPar
As it is markdown, you can embed images, HTML, etc into your posts!

\sphinxAtStartPar
\sphinxincludegraphics{{/Users/jerzybaranowski/GitHub/Private/ksiazka_profesorska/bayes_book/_build/.doctrees/images/86f925bcfc7b65561516d54179f9c6a9096d9072/logo-wide}.svg}

\sphinxAtStartPar
You can also \(add_{math}\) and
\begin{equation*}
\begin{split}
math^{blocks}
\end{split}
\end{equation*}
\sphinxAtStartPar
or
\begin{equation*}
\begin{split}
\begin{aligned}
\mbox{mean} la_{tex} \\ \\
math blocks
\end{aligned}
\end{split}
\end{equation*}
\sphinxAtStartPar
But make sure you \$Escape \$your \$dollar signs \$you want to keep!


\section{MyST markdown}
\label{\detokenize{notebooks:myst-markdown}}
\sphinxAtStartPar
MyST markdown works in Jupyter Notebooks as well. For more information about MyST markdown, check
out \sphinxhref{https://jupyterbook.org/content/myst.html}{the MyST guide in Jupyter Book},
or see \sphinxhref{https://myst-parser.readthedocs.io/en/latest/}{the MyST markdown documentation}.


\section{Code blocks and outputs}
\label{\detokenize{notebooks:code-blocks-and-outputs}}
\sphinxAtStartPar
Jupyter Book will also embed your code blocks and output in your book.
For example, here’s some sample Matplotlib code:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{rcParams}\PYG{p}{,} \PYG{n}{cycler}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ion}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.pyplot.\PYGZus{}IonContext at 0x7fd1613e6b30\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Fixing random state for reproducibility}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{19680801}\PYG{p}{)}

\PYG{n}{N} \PYG{o}{=} \PYG{l+m+mi}{10}
\PYG{n}{data} \PYG{o}{=} \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{logspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{)} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{)} \PYG{o}{+} \PYG{n}{ii} \PYG{k}{for} \PYG{n}{ii} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{N}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}
\PYG{n}{cmap} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{cm}\PYG{o}{.}\PYG{n}{coolwarm}
\PYG{n}{rcParams}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{axes.prop\PYGZus{}cycle}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{cycler}\PYG{p}{(}\PYG{n}{color}\PYG{o}{=}\PYG{n}{cmap}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{N}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}


\PYG{k+kn}{from} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{lines} \PYG{k+kn}{import} \PYG{n}{Line2D}
\PYG{n}{custom\PYGZus{}lines} \PYG{o}{=} \PYG{p}{[}\PYG{n}{Line2D}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{n}{cmap}\PYG{p}{(}\PYG{l+m+mf}{0.}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}
                \PYG{n}{Line2D}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{n}{cmap}\PYG{p}{(}\PYG{l+m+mf}{.5}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}
                \PYG{n}{Line2D}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{n}{cmap}\PYG{p}{(}\PYG{l+m+mf}{1.}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{]}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{lines} \PYG{o}{=} \PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{n}{custom\PYGZus{}lines}\PYG{p}{,} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cold}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Medium}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hot}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{notebooks_2_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
There is a lot more that you can do with outputs (such as including interactive outputs)
with your book. For more information about this, see \sphinxhref{https://jupyterbook.org}{the Jupyter Book documentation}


\chapter{Notebooks with MyST Markdown}
\label{\detokenize{markdown-notebooks:notebooks-with-myst-markdown}}\label{\detokenize{markdown-notebooks::doc}}
\sphinxAtStartPar
Jupyter Book also lets you write text\sphinxhyphen{}based notebooks using MyST Markdown.
See \sphinxhref{https://jupyterbook.org/file-types/myst-notebooks.html}{the Notebooks with MyST Markdown documentation} for more detailed instructions.
This page shows off a notebook written in MyST Markdown.


\section{An example cell}
\label{\detokenize{markdown-notebooks:an-example-cell}}
\sphinxAtStartPar
With MyST Markdown, you can define code cells with a directive like so:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+m+mi}{2} \PYG{o}{+} \PYG{l+m+mi}{2}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
4
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
When your book is built, the contents of any \sphinxcode{\sphinxupquote{\{code\sphinxhyphen{}cell\}}} blocks will be
executed with your default Jupyter kernel, and their outputs will be displayed
in\sphinxhyphen{}line with the rest of your content.


\sphinxstrong{See also:}
\nopagebreak


\sphinxAtStartPar
Jupyter Book uses \sphinxhref{https://jupytext.readthedocs.io/en/latest/}{Jupytext} to convert text\sphinxhyphen{}based files to notebooks, and can support \sphinxhref{https://jupyterbook.org/file-types/jupytext.html}{many other text\sphinxhyphen{}based notebook files}.




\section{Create a notebook with MyST Markdown}
\label{\detokenize{markdown-notebooks:create-a-notebook-with-myst-markdown}}
\sphinxAtStartPar
MyST Markdown notebooks are defined by two things:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
YAML metadata that is needed to understand if / how it should convert text files to notebooks (including information about the kernel needed).
See the YAML at the top of this page for example.

\item {} 
\sphinxAtStartPar
The presence of \sphinxcode{\sphinxupquote{\{code\sphinxhyphen{}cell\}}} directives, which will be executed with your book.

\end{enumerate}

\sphinxAtStartPar
That’s all that is needed to get started!


\section{Quickly add YAML metadata for MyST Notebooks}
\label{\detokenize{markdown-notebooks:quickly-add-yaml-metadata-for-myst-notebooks}}
\sphinxAtStartPar
If you have a markdown file and you’d like to quickly add YAML metadata to it, so that Jupyter Book will treat it as a MyST Markdown Notebook, run the following command:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{jupyter}\PYG{o}{\PYGZhy{}}\PYG{n}{book} \PYG{n}{myst} \PYG{n}{init} \PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{markdownfile}\PYG{o}{.}\PYG{n}{md}
\end{sphinxVerbatim}

\begin{sphinxthebibliography}{HdHPK14}
\bibitem[HdHPK14]{markdown:id3}
\sphinxAtStartPar
Christopher Ramsay Holdgraf, Wendy de Heer, Brian N. Pasley, and Robert T. Knight. Evidence for Predictive Coding in Human Auditory Cortex. In \sphinxstyleemphasis{International Conference on Cognitive Neuroscience}. Brisbane, Australia, Australia, 2014. Frontiers in Neuroscience.
\end{sphinxthebibliography}







\renewcommand{\indexname}{Index}
\printindex
\end{document}